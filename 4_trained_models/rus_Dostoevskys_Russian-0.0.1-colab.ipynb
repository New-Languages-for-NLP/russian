{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061ba15a"
      },
      "source": [
        "<img width=50% src=\"https://github.com/New-Languages-for-NLP/course-materials/raw/main/w2/using-inception-data/newnlp_notebook.png\" />\n",
        "\n",
        "For full documentation on this project, see [here](https://new-languages-for-nlp.github.io/course-materials/w2/using-inception-data/New%20Language%20Training.html)\n",
        " "
      ],
      "id": "061ba15a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psP7JdcRLR7n"
      },
      "source": [
        "# 1 Prepare the Notebook Environment"
      ],
      "id": "psP7JdcRLR7n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0ba9e5a",
        "outputId": "668e7c49-f237-47b1-99a7-ec92abf52cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.0 MB 19.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 40.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 76.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 170 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 62.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title The Colab runtime comes with spaCy v2 and needs to be upgraded to v3.\n",
        "#@markdown This project uses the GPU by default, if you need to use just the CPU, just uncheck the box below.\n",
        "GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Install spaCy v3 and libraries for GPUs and transformers\n",
        "!pip install spacy --upgrade --quiet\n",
        "if GPU:\n",
        "    !pip install 'spacy[transformers,cuda111]' --quiet\n",
        "#!pip install wandb spacy-huggingface-hub --quiet"
      ],
      "id": "a0ba9e5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfsEKZv6ErlG"
      },
      "source": [
        "The notebook will pull project files from your GitHub repository.  \n",
        "\n",
        "Note that you need to set the langugage (lang), treebank (same as the repo name), test_size and package name in the project.yml file in your repository.  "
      ],
      "id": "WfsEKZv6ErlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c0bda8e",
        "outputId": "60695110-5217-4e57-e28f-945d890da5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'newlang_project' from New-Languages-for-NLP/russian\u001b[0m\n",
            "/content/newlang_project\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/newlang_project\n",
            "\u001b[38;5;4mℹ Fetching 1 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/newlang_project/assets/russian\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Enter your language's repository name. \n",
        "#@markdown If the repo is private, please check the \"private_repo\" box and include an [access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n",
        "private_repo = False #@param {type:\"boolean\"}\n",
        "repo_name = \"russian\" #@param {type:\"string\"}\n",
        "branch = \"main\"\n",
        "\n",
        "\n",
        "!rm -rf /content/newlang_project\n",
        "!rm -rf $repo_name\n",
        "if private_repo:\n",
        "    git_access_token = \"\" #@param {type:\"string\"}\n",
        "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
        "    !git clone $git_url  -b $branch\n",
        "    !cp -r ./$repo_name/newlang_project .  \n",
        "    !mkdir newlang_project/assets/\n",
        "    !mkdir newlang_project/configs/\n",
        "    #!mkdir newlang_project/corpus/\n",
        "    !mkdir newlang_project/metrics/\n",
        "    !mkdir newlang_project/packages/\n",
        "    !mkdir newlang_project/training/\n",
        "    !mkdir newlang_project/assets/$repo_name\n",
        "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
        "    !rm -rf ./$repo_name\n",
        "else:\n",
        "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch $branch\n",
        "    !python -m spacy project assets /content/newlang_project"
      ],
      "id": "7c0bda8e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dc13741",
        "outputId": "e9b275db-2b3b-41b7-aabc-557714c0325b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== install ==================================\u001b[0m\n",
            "Running command: rm -rf lang\n",
            "Running command: mkdir lang\n",
            "Running command: mkdir lang/rus\n",
            "Running command: cp -r assets/russian/2_new_language_object/ lang/rus/rus\n",
            "Running command: mv lang/rus/rus/setup.py lang/rus/\n",
            "Running command: /usr/bin/python3 -m pip install -e lang/rus\n",
            "Obtaining file:///content/newlang_project/lang/rus\n",
            "Installing collected packages: rus\n",
            "  Running setup.py develop for rus\n",
            "Successfully installed rus-0.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the custom language object from Cadet \n",
        "!python -m spacy project run install /content/newlang_project"
      ],
      "id": "4dc13741"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU_AqRK6LZrF"
      },
      "source": [
        "# 2 Prepare the Data for Training"
      ],
      "id": "qU_AqRK6LZrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjD0SN6iAvc",
        "outputId": "39991c1b-0cf3-415d-99fc-bd9855d21537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n"
          ]
        }
      ],
      "source": [
        "#@title (optional) cell to correct a problem when your tokens have no pos value\n",
        "%%writefile /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n",
        "import re\n",
        "\n",
        "from .conll_ner_to_docs import n_sents_info\n",
        "from ...training import iob_to_biluo, biluo_tags_to_spans\n",
        "from ...tokens import Doc, Token, Span\n",
        "from ...vocab import Vocab\n",
        "from wasabi import Printer\n",
        "\n",
        "\n",
        "def conllu_to_docs(\n",
        "    input_data,\n",
        "    n_sents=10,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "    merge_subtokens=False,\n",
        "    no_print=False,\n",
        "    **_\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert conllu files into JSON format for use with train cli.\n",
        "    append_morphology parameter enables appending morphology to tags, which is\n",
        "    useful for languages such as Spanish, where UD tags are not so rich.\n",
        "\n",
        "    Extract NER tags if available and convert them so that they follow\n",
        "    BILUO and the Wikipedia scheme\n",
        "    \"\"\"\n",
        "    MISC_NER_PATTERN = \"^((?:name|NE)=)?([BILU])-([A-Z_]+)|O$\"\n",
        "    msg = Printer(no_print=no_print)\n",
        "    n_sents_info(msg, n_sents)\n",
        "    sent_docs = read_conllx(\n",
        "        input_data,\n",
        "        append_morphology=append_morphology,\n",
        "        ner_tag_pattern=MISC_NER_PATTERN,\n",
        "        ner_map=ner_map,\n",
        "        merge_subtokens=merge_subtokens,\n",
        "    )\n",
        "    sent_docs_to_merge = []\n",
        "    for sent_doc in sent_docs:\n",
        "        sent_docs_to_merge.append(sent_doc)\n",
        "        if len(sent_docs_to_merge) % n_sents == 0:\n",
        "            yield Doc.from_docs(sent_docs_to_merge)\n",
        "            sent_docs_to_merge = []\n",
        "    if sent_docs_to_merge:\n",
        "        yield Doc.from_docs(sent_docs_to_merge)\n",
        "\n",
        "\n",
        "def has_ner(input_data, ner_tag_pattern):\n",
        "    \"\"\"\n",
        "    Check the MISC column for NER tags.\n",
        "    \"\"\"\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            for line in lines:\n",
        "                parts = line.split(\"\\t\")\n",
        "                id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "                for misc_part in misc.split(\"|\"):\n",
        "                    if re.match(ner_tag_pattern, misc_part):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def read_conllx(\n",
        "    input_data,\n",
        "    append_morphology=False,\n",
        "    merge_subtokens=False,\n",
        "    ner_tag_pattern=\"\",\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Yield docs, one for each sentence\"\"\"\n",
        "    vocab = Vocab()  # need vocab to make a minimal Doc\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            doc = conllu_sentence_to_doc(\n",
        "                vocab,\n",
        "                lines,\n",
        "                ner_tag_pattern,\n",
        "                merge_subtokens=merge_subtokens,\n",
        "                append_morphology=append_morphology,\n",
        "                ner_map=ner_map,\n",
        "            )\n",
        "            yield doc\n",
        "\n",
        "\n",
        "def get_entities(lines, tag_pattern, ner_map=None):\n",
        "    \"\"\"Find entities in the MISC column according to the pattern and map to\n",
        "    final entity type with `ner_map` if mapping present. Entity tag is 'O' if\n",
        "    the pattern is not matched.\n",
        "\n",
        "    lines (str): CONLL-U lines for one sentences\n",
        "    tag_pattern (str): Regex pattern for entity tag\n",
        "    ner_map (dict): Map old NER tag names to new ones, '' maps to O.\n",
        "    RETURNS (list): List of BILUO entity tags\n",
        "    \"\"\"\n",
        "    miscs = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_ or \".\" in id_:\n",
        "            continue\n",
        "        miscs.append(misc)\n",
        "\n",
        "    iob = []\n",
        "    for misc in miscs:\n",
        "        iob_tag = \"O\"\n",
        "        for misc_part in misc.split(\"|\"):\n",
        "            tag_match = re.match(tag_pattern, misc_part)\n",
        "            if tag_match:\n",
        "                prefix = tag_match.group(2)\n",
        "                suffix = tag_match.group(3)\n",
        "                if prefix and suffix:\n",
        "                    iob_tag = prefix + \"-\" + suffix\n",
        "                    if ner_map:\n",
        "                        suffix = ner_map.get(suffix, suffix)\n",
        "                        if suffix == \"\":\n",
        "                            iob_tag = \"O\"\n",
        "                        else:\n",
        "                            iob_tag = prefix + \"-\" + suffix\n",
        "                break\n",
        "        iob.append(iob_tag)\n",
        "    return iob_to_biluo(iob)\n",
        "\n",
        "\n",
        "def conllu_sentence_to_doc(\n",
        "    vocab,\n",
        "    lines,\n",
        "    ner_tag_pattern,\n",
        "    merge_subtokens=False,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Create an Example from the lines for one CoNLL-U sentence, merging\n",
        "    subtokens and appending morphology to tags if required.\n",
        "\n",
        "    lines (str): The non-comment lines for a CoNLL-U sentence\n",
        "    ner_tag_pattern (str): The regex pattern for matching NER in MISC col\n",
        "    RETURNS (Example): An example containing the annotation\n",
        "    \"\"\"\n",
        "    # create a Doc with each subtoken as its own token\n",
        "    # if merging subtokens, each subtoken orth is the merged subtoken form\n",
        "    if not Token.has_extension(\"merged_orth\"):\n",
        "        Token.set_extension(\"merged_orth\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_lemma\"):\n",
        "        Token.set_extension(\"merged_lemma\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_morph\"):\n",
        "        Token.set_extension(\"merged_morph\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_spaceafter\"):\n",
        "        Token.set_extension(\"merged_spaceafter\", default=\"\")\n",
        "    words, spaces, tags, poses, morphs, lemmas = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    subtok_word = \"\"\n",
        "    in_subtok = False\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \".\" in id_:\n",
        "            continue\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "            subtok_word = word\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_spaceafter = \"SpaceAfter=No\" not in misc\n",
        "            continue\n",
        "        if merge_subtokens and in_subtok:\n",
        "            words.append(subtok_word)\n",
        "        else:\n",
        "            words.append(word)\n",
        "        if in_subtok:\n",
        "            if id_ == subtok_end:\n",
        "                spaces.append(subtok_spaceafter)\n",
        "            else:\n",
        "                spaces.append(False)\n",
        "        elif \"SpaceAfter=No\" in misc:\n",
        "            spaces.append(False)\n",
        "        else:\n",
        "            spaces.append(True)\n",
        "        if in_subtok and id_ == subtok_end:\n",
        "            subtok_word = \"\"\n",
        "            in_subtok = False\n",
        "        id_ = int(id_) - 1\n",
        "        head = (int(head) - 1) if head not in (\"0\", \"_\") else id_\n",
        "        tag = pos if tag == \"_\" else tag\n",
        "        morph = morph if morph != \"_\" else \"\"\n",
        "        dep = \"ROOT\" if dep == \"root\" else dep\n",
        "        lemmas.append(lemma)\n",
        "        if pos == \"_\":\n",
        "            pos = \"\"\n",
        "        poses.append(pos)\n",
        "        tags.append(tag)\n",
        "        morphs.append(morph)\n",
        "        heads.append(head)\n",
        "        deps.append(dep)\n",
        "\n",
        "    doc = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        lemmas=lemmas,\n",
        "        morphs=morphs,\n",
        "        heads=heads,\n",
        "    )\n",
        "    for i in range(len(doc)):\n",
        "        doc[i]._.merged_orth = words[i]\n",
        "        doc[i]._.merged_morph = morphs[i]\n",
        "        doc[i]._.merged_lemma = lemmas[i]\n",
        "        doc[i]._.merged_spaceafter = spaces[i]\n",
        "    ents = get_entities(lines, ner_tag_pattern, ner_map)\n",
        "    doc.ents = biluo_tags_to_spans(doc, ents)\n",
        "\n",
        "    if merge_subtokens:\n",
        "        doc = merge_conllu_subtokens(lines, doc)\n",
        "\n",
        "    # create final Doc from custom Doc annotation\n",
        "    words, spaces, tags, morphs, lemmas, poses = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    for i, t in enumerate(doc):\n",
        "        words.append(t._.merged_orth)\n",
        "        lemmas.append(t._.merged_lemma)\n",
        "        spaces.append(t._.merged_spaceafter)\n",
        "        morphs.append(t._.merged_morph)\n",
        "        if append_morphology and t._.merged_morph:\n",
        "            tags.append(t.tag_ + \"__\" + t._.merged_morph)\n",
        "        else:\n",
        "            tags.append(t.tag_)\n",
        "        poses.append(t.pos_)\n",
        "        heads.append(t.head.i)\n",
        "        deps.append(t.dep_)\n",
        "\n",
        "    doc_x = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        morphs=morphs,\n",
        "        lemmas=lemmas,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        heads=heads,\n",
        "    )\n",
        "    doc_x.ents = [Span(doc_x, ent.start, ent.end, label=ent.label) for ent in doc.ents]\n",
        "\n",
        "    return doc_x\n",
        "\n",
        "\n",
        "def merge_conllu_subtokens(lines, doc):\n",
        "    # identify and process all subtoken spans to prepare attrs for merging\n",
        "    subtok_spans = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_:\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_span = doc[int(subtok_start) - 1 : int(subtok_end)]\n",
        "            subtok_spans.append(subtok_span)\n",
        "            # create merged tag, morph, and lemma values\n",
        "            tags = []\n",
        "            morphs = {}\n",
        "            lemmas = []\n",
        "            for token in subtok_span:\n",
        "                tags.append(token.tag_)\n",
        "                lemmas.append(token.lemma_)\n",
        "                if token._.merged_morph:\n",
        "                    for feature in token._.merged_morph.split(\"|\"):\n",
        "                        field, values = feature.split(\"=\", 1)\n",
        "                        if field not in morphs:\n",
        "                            morphs[field] = set()\n",
        "                        for value in values.split(\",\"):\n",
        "                            morphs[field].add(value)\n",
        "            # create merged features for each morph field\n",
        "            for field, values in morphs.items():\n",
        "                morphs[field] = field + \"=\" + \",\".join(sorted(values))\n",
        "            # set the same attrs on all subtok tokens so that whatever head the\n",
        "            # retokenizer chooses, the final attrs are available on that token\n",
        "            for token in subtok_span:\n",
        "                token._.merged_orth = token.orth_\n",
        "                token._.merged_lemma = \" \".join(lemmas)\n",
        "                token.tag_ = \"_\".join(tags)\n",
        "                token._.merged_morph = \"|\".join(sorted(morphs.values()))\n",
        "                token._.merged_spaceafter = (\n",
        "                    True if subtok_span[-1].whitespace_ else False\n",
        "                )\n",
        "\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in subtok_spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    return doc"
      ],
      "id": "HPjD0SN6iAvc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563fdc94",
        "outputId": "ea2d8c20-0e5d-4dd2-8875-7b2ab96ce19e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== convert ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/convert.py assets/russian/3_inception_export 10 rus\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishmentsample2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents): corpus/conllu/demons-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/doublesample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/karamazov-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/demonssample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/underground-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/double-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazovsample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/karamazov-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/crimepunishment-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/idiot-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conllu/demons-0.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/underground-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents): corpus/conllu/double-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/crimepunishment-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/crimepunishment-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/underground-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazovsample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents): corpus/conll/demons-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/doublesample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/demonssample.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/crimepunishment-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-0.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/crimepunishment-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishmentsample2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conll/karamazov-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/demons-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-1.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-4.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/double-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/underground-8.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-5.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-6.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-7.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents): corpus/conll/double-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/crimepunishment-9.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conll/karamazov-3.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-2.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): corpus/conll/idiot-1.spacy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Convert the conllu files from inception to spaCy binary format\n",
        "# Read the conll files with ner data and as ents to spaCy docs \n",
        "!python -m spacy project run convert /content/newlang_project"
      ],
      "id": "563fdc94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9519c858",
        "outputId": "22a5f668-79a7-4052-cac8-8dc7091db6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== split ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/split.py 0.2 11 rus\n",
            "🚂 Created 52 training docs\n",
            "😊 Created 10 validation docs\n",
            "🧪  Created 3 test docs\n"
          ]
        }
      ],
      "source": [
        "# test/train split \n",
        "!python -m spacy project run split /content/newlang_project "
      ],
      "id": "9519c858"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4feefe6f",
        "outputId": "52b963c8-aa2d-404f-9bb0-7db3065ef8df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== debug ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy debug data configs/config.cfg\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: rus\n",
            "Training pipeline: tok2vec, tagger, parser, ner\n",
            "52 training docs\n",
            "10 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;1m✘ Low number of examples to train a new pipeline (52)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 17361 total word(s) in the data (4651 unique)\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 3 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'null' (10)\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 17 label(s) in train data\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 17360 sentence(s) with an average length of 1.0 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 1 warning\u001b[0m\n",
            "\u001b[38;5;1m✘ 1 error\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Debug the data\n",
        "!python -m spacy project run debug /content/newlang_project "
      ],
      "id": "4feefe6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "151-cj1dLgAD"
      },
      "source": [
        "# 3 Model Training "
      ],
      "id": "151-cj1dLgAD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW2QGPTJ_CZ"
      },
      "source": [
        "If your project file uses Weights and Biases to monitor model training (`vars.wanb: true`), you'll need to create an account at [wandb.ai](https://wandb.ai/site) and get an API key.  "
      ],
      "id": "KKW2QGPTJ_CZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38b490a4",
        "outputId": "dee577b1-7a75-487e-db78-07f61a79efe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config.cfg --output training/russian --gpu-id 0 --nlp.lang=rus\n",
            "\u001b[38;5;2m✔ Created output directory: training/russian\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/russian\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-14 16:58:43,086] [INFO] Set up nlp object from config\n",
            "[2022-01-14 16:58:43,097] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "[2022-01-14 16:58:43,102] [INFO] Created vocabulary\n",
            "[2022-01-14 16:58:43,103] [INFO] Finished initializing nlp object\n",
            "[2022-01-14 16:58:57,812] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "  0       0          0.00       751.06         0.00    382.14    34.35   100.00     0.00   100.00    9.89    6.81   18.04    0.32\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "('L-dep', 0, 9000.0)\n",
            "('R-dep', 0, 9000.0)\n",
            "('B-_', 0, 9000.0)\n",
            "('B-ROOT', 0, 9000.0)\n",
            "('Gold sent starts?', 1, 1)\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "ValueError('Could not find gold transition - see logs above.')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1153, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/transition_parser.pyx\", line 406, in spacy.pipeline.transition_parser.Parser.update\n",
            "  File \"spacy/pipeline/transition_parser.pyx\", line 518, in spacy.pipeline.transition_parser.Parser.get_batch_loss\n",
            "  File \"spacy/pipeline/_parser_internals/arc_eager.pyx\", line 827, in spacy.pipeline._parser_internals.arc_eager.ArcEager.set_costs\n",
            "ValueError: Could not find gold transition - see logs above.\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "!python -m spacy project run train /content/newlang_project "
      ],
      "id": "38b490a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynXr8vlXqCxv"
      },
      "source": [
        "If you get `ValueError: Could not find gold transition - see logs above.`  \n",
        "You may not have sufficent data to train on: https://github.com/explosion/spaCy/discussions/7282"
      ],
      "id": "ynXr8vlXqCxv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "018362d8",
        "outputId": "2f6250b8-82ba-4c15-843f-d4d3fb79101d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate ./training/russian/model-best ./corpus/converted/test.spacy --output ./metrics/russian.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK      100.00\n",
            "TAG      33.26 \n",
            "UAS      100.00\n",
            "LAS      0.00  \n",
            "NER P    6.80  \n",
            "NER R    13.89 \n",
            "NER F    9.13  \n",
            "SENT P   100.00\n",
            "SENT R   100.00\n",
            "SENT F   100.00\n",
            "SPEED    1762  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== LAS (per type) ===============================\u001b[0m\n",
            "\n",
            "          P      R      F\n",
            "_      0.00   0.00   0.00\n",
            "root   0.00   0.00   0.00\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "         P       R      F\n",
            "PER   6.80   15.87   9.52\n",
            "FAC   0.00    0.00   0.00\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics/russian.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the test data\n",
        "!python -m spacy project run evaluate /content/newlang_project "
      ],
      "id": "018362d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg1sLlVrgiyu",
        "outputId": "512c1b80-9b07-4512-d3a9-3bf384cad83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model-best  model-last\n"
          ]
        }
      ],
      "source": [
        "# Find the path for your meta.json file\n",
        "# You'll need to add newlang_project/ +  the path from the training step just after \"✔ Saved pipeline to output directory\"\n",
        "!ls newlang_project/training/russian/"
      ],
      "id": "gg1sLlVrgiyu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zGCTURr9JE6"
      },
      "outputs": [],
      "source": [
        "#Update meta.json\n",
        "import spacy \n",
        "import srsly \n",
        "\n",
        "# Change path to match that from the training cell where it says \"✔ Saved pipeline to output directory\"\n",
        "meta_path = \"newlang_project/training/russian/model-last/meta.json\"\n",
        "\n",
        "# Replace values below for your project\n",
        "my_meta = { \n",
        "    \"lang\":\"rus\",\n",
        "    \"name\":\"Dostoevskys_Russian\",\n",
        "    \"version\":\"0.0.1\",\n",
        "    \"description\":\"Russian pipeline optimized for GPU. Components: tok2vec, tagger, parser, senter, lemmatizer.\",\n",
        "    \"author\":\"New Languages for NLP\",\n",
        "    \"email\":\"newnlp@princeton.edu\",\n",
        "    \"url\":\"https://newnlp.princeton.edu\",\n",
        "    \"license\":\"MIT\", \n",
        "    }\n",
        "meta = spacy.util.load_meta(meta_path)\n",
        "meta.update(my_meta)\n",
        "srsly.write_json(meta_path, meta)"
      ],
      "id": "3zGCTURr9JE6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM309FhNVAeb"
      },
      "source": [
        "### Download the trained model to your computer.\n"
      ],
      "id": "JM309FhNVAeb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1d6f36",
        "outputId": "7d1514af-4f07-4c1f-f001-49f127e1009a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Building package artifacts: sdist\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "newlang_project/training/russian/model-last/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package 'rus_Dostoevskys_Russian-0.0.1'\u001b[0m\n",
            "newlang_project/export/rus_Dostoevskys_Russian-0.0.1\n",
            "running sdist\n",
            "running egg_info\n",
            "creating rus_Dostoevskys_Russian.egg-info\n",
            "writing rus_Dostoevskys_Russian.egg-info/PKG-INFO\n",
            "writing dependency_links to rus_Dostoevskys_Russian.egg-info/dependency_links.txt\n",
            "writing entry points to rus_Dostoevskys_Russian.egg-info/entry_points.txt\n",
            "writing requirements to rus_Dostoevskys_Russian.egg-info/requires.txt\n",
            "writing top-level names to rus_Dostoevskys_Russian.egg-info/top_level.txt\n",
            "writing manifest file 'rus_Dostoevskys_Russian.egg-info/SOURCES.txt'\n",
            "reading manifest file 'rus_Dostoevskys_Russian.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'rus_Dostoevskys_Russian.egg-info/SOURCES.txt'\n",
            "running check\n",
            "creating rus_Dostoevskys_Russian-0.0.1\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tagger\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tok2vec\n",
            "creating rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "copying files to rus_Dostoevskys_Russian-0.0.1...\n",
            "copying MANIFEST.in -> rus_Dostoevskys_Russian-0.0.1\n",
            "copying README.md -> rus_Dostoevskys_Russian-0.0.1\n",
            "copying meta.json -> rus_Dostoevskys_Russian-0.0.1\n",
            "copying setup.py -> rus_Dostoevskys_Russian-0.0.1\n",
            "copying rus_Dostoevskys_Russian/__init__.py -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian\n",
            "copying rus_Dostoevskys_Russian/meta.json -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian\n",
            "copying rus_Dostoevskys_Russian.egg-info/PKG-INFO -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/SOURCES.txt -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/dependency_links.txt -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/entry_points.txt -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/not-zip-safe -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/requires.txt -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian.egg-info/top_level.txt -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian.egg-info\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/README.md -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/config.cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/meta.json -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tokenizer -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner/cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner/model -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner/moves -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/ner\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser/cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser/model -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser/moves -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/parser\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tagger/cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tagger\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tagger/model -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tagger\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tok2vec/cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tok2vec\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tok2vec/model -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/tok2vec\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab/key2row -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab/lookups.bin -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab/strings.json -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab/vectors -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "copying rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab/vectors.cfg -> rus_Dostoevskys_Russian-0.0.1/rus_Dostoevskys_Russian/rus_Dostoevskys_Russian-0.0.1/vocab\n",
            "Writing rus_Dostoevskys_Russian-0.0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'rus_Dostoevskys_Russian-0.0.1' (and everything under it)\n",
            "\u001b[38;5;2m✔ Successfully created zipped Python package\u001b[0m\n",
            "newlang_project/export/rus_Dostoevskys_Russian-0.0.1/dist/rus_Dostoevskys_Russian-0.0.1.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Save the model to disk in a format that can be easily  downloaded and re-used.\n",
        "!python -m spacy package newlang_project/training/russian/model-last newlang_project/export "
      ],
      "id": "8e1d6f36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8d32abf2",
        "outputId": "777fd731-25c0-459a-90a6-88332eb13b0d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_398cae8d-3fc1-4b6b-80fc-f15a466fbd50\", \"rus_Dostoevskys_Russian-0.0.1.tar.gz\", 7739071)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# replace with the path in the previous cell under \"✔ Successfully created zipped Python package\"\n",
        "files.download('newlang_project/export/rus_Dostoevskys_Russian-0.0.1/dist/rus_Dostoevskys_Russian-0.0.1.tar.gz')\n",
        "\n",
        "# once on your computer, you can pip install yi_yiddish_sm-0.0.1.tar.gz\n",
        "# Be sure to add the file to the 4_trained_models folder in GitHub"
      ],
      "id": "8d32abf2"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of New Language Training (Colab).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}